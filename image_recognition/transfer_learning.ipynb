{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning\n",
    "\n",
    "### Transfer learning is the process of taking a pretrained Neural Network and using it for our own use.\n",
    "In this process, we are loading a pretrained neural network, removing the flatten and output layers so we'll have a network that is trained only to extract features from image.\n",
    "\n",
    "Then, we are adding layers to it for our use - like falttening and output according to our dataset - and then we train the model on our dataset.\n",
    "\n",
    "This way, the pretrained network performs the complex task of feature extraction and we are adding the simple task of labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "import joblib\n",
    "from keras.preprocessing import image\n",
    "from keras.applications import vgg16\n",
    "import keras\n",
    "from keras.models import Sequential,model_from_json\n",
    "from keras.layers import Dense,Dropout,Flatten\n",
    "\n",
    "base_path = \"datasets\"\n",
    "dog_path = Path(os.path.join(base_path,\"dogs\"))\n",
    "not_dog_path = Path(os.path.join(base_path,\"not_dogs\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "labels = []\n",
    "# This function addes data to images and labels arrays\n",
    "# path - a path for a Path object directory\n",
    "# label - the number represents the label for the dir\n",
    "def add_data_to_arrays(path,label):\n",
    "    # for each image in the path\n",
    "    for img in path.glob(\"*.png\"):\n",
    "        # load the image to an array\n",
    "        img = keras.utils.load_img(img)\n",
    "        img_array = image.image_utils.img_to_array(img=img)\n",
    "        # append the image and the labels to the arrays\n",
    "        images.append(img_array)\n",
    "        labels.append(label)\n",
    "add_data_to_arrays(not_dog_path,0)\n",
    "add_data_to_arrays(dog_path,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the data to numpy arrays\n",
    "x_train = np.array(images)\n",
    "y_train = np.array(labels)\n",
    "# perform preprocessing on the x data\n",
    "x_train = vgg16.preprocess_input(x_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This loads a pretrained vgg16 CNN structure with the weights of a network trained by imagenet dataset\n",
    "# The include_top=False means that we don't want to include the flattening output layer, because in transfer learning we are using the CNN only for feature extraction\n",
    "pretrained_nn = vgg16.VGG16(weights=\"imagenet\",include_top=False,input_shape=(64,64,3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 14 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001F02DA576D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 1s 210ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['y_train.dat']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract features of each image in one pass - the predict this time finishes in the last Convolutional layer so we'll get only the features of each image\n",
    "features_x = pretrained_nn.predict(x_train)\n",
    "# Dumping the results in data files\n",
    "joblib.dump(features_x,\"x_train.dat\")\n",
    "joblib.dump(y_train,\"y_train.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = joblib.load(\"x_train.dat\")\n",
    "y_train = joblib.load(\"y_train.dat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model\n",
    "\n",
    "Since we are using transfer learning, once we used the pretrained model for creating data, we'll use another model to label this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Flatten(input_shape=(x_train.shape[1:])))\n",
    "model.add(Dense(256,activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer='Adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 12.2340 - accuracy: 0.4655\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.0361 - accuracy: 0.9138\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.2122e-09 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.2585 - accuracy: 0.9828\n",
      "Epoch 5/10\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0327 - accuracy: 0.9828\n",
      "Epoch 6/10\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.3547 - accuracy: 0.9828\n",
      "Epoch 7/10\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.3005 - accuracy: 0.9828\n",
      "Epoch 8/10\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.7064e-08 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.1733 - accuracy: 0.9828\n",
      "Epoch 10/10\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.2785e-13 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f02df3fb80>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=x_train,y=y_train,epochs=10,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_structure = model.to_json()\n",
    "f = Path(\"model_structure_2.json\")\n",
    "f.write_text(model_structure)\n",
    "model.save_weights(\"model_weights_2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "f = Path(\"model_structure_2.json\")\n",
    "model_structure = f.read_text()\n",
    "model = model_from_json(model_structure)\n",
    "\n",
    "model.load_weights(\"model_weights_2.h5\")\n",
    "\n",
    "img = keras.utils.load_img(\"dog.png\",target_size=(64,64))\n",
    "img_array = image.image_utils.img_to_array(img)\n",
    "images = np.expand_dims(img_array,axis=0)\n",
    "images = vgg16.preprocess_input(images)\n",
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# feature_extraction_model = vgg16.VGG16(weights=\"imagenet\",include_top=False,input_shape=(64,64,3))\n",
    "\n",
    "features = pretrained_nn.predict(images)\n",
    "\n",
    "results = model.predict(features)\n",
    "\n",
    "result_likelihood = results[0][0]\n",
    "\n",
    "print(result_likelihood)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4a68054a1377440dd3f78f1f273580d7335c2e20b564bc770fee5dc8903a7f04"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
